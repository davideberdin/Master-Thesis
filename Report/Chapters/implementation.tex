%!TEX root = ../thesis.tex
\chapter{Implementation}
\label{chap:Implementation}
In this chapter we explain the infrastructure that performs all the necessary steps to produce efficient feedback. A general overview is given and for each section, we describe in particular the tools as well as the way we manipulated the data in order to obtain the information useful for the user. The chapter is divided in two parts: the first part focuses on the back-end and the services we used to extract the features we described in chapter~\ref{chap:Speech Recognition}. The second part describe the front-end, that is, the \textit{Android}\footnote{\url{https://www.android.com}} application (called \textbf{PARLA}\footnote{\url{https://github.com/davideberdin/PARLA}}) with a particular focus on the feedback page and the general usage.

\section{General architecture}
\label{sec:general_architecture}

In \ref{fig:general_architecture} the general architecture of the infrastructure is shown.
The flow displays only the \textit{pronunciation testing} phase:

\begin{itemize}
	\item[1)] User says the sentence using the internal microphone of the smartphone (or through the headset)
	\item[2)] The application sends the audio file to the \textit{Speech Recognition service}
	\item[3)] The result of step 2 is sent to the \textit{Gaussian Mixture Model service} (or \textit{Audio Analysis Service})
	\item[4)] The result of step 3 is sent back to the application where a \textit{Feedback page} is displayed
	\item[5)] A short explanation for each chart is given to the user
	\item[6)] Back to step 1
\end{itemize}

\noindent The flow described above is the main feature of the whole project, although, the application also supplies two other important functionalities that are described more in detail in section~\ref{sec:android_app}. The first one is related to \textbf{critical listening} where the user is able to listen to the \textit{native pronunciation} as well as to their own. This feature has a big impact on improving the pronunciation because it pushes the user to understand the differences as well as to emulate the way native speakers pronounce a specific sequence of words. The second feature regards the \textbf{history} (or progress). This page shows the trend of the user based on all the pronunciation he/she made during the usage of PARLA. The purpose of the history page is to help the user to see their progress and to get an idea of how to improve the pronunciation. \\

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.6]{Figures/general_architecture.png}
	\caption{General architecture of the infrastructure}
	\label{fig:general_architecture}
\end{figure}

\subsubsection{Implementation procedure}
\label{ssec:procedure}

Several steps were made before reaching the architecture depicted in Figure~\ref{fig:general_architecture}. Generally speaking, the implementation was divided into two main categories: the first is composed of the \textit{data collection and training} phase whereas the second is formed by the \textit{mobile application} and \textit{server communication}. \\

\noindent The very first step was to collect the data from native speakers and apply some pre-processing techniques in such a way that we were able to obtain only the information we needed to train the two services we had on the server. After the data collection, we trained both the models with the information we extracted in the previous step. The detailed procedures are described in sections~\ref{ssec:training_sr_model} and \ref{ssec:training_gmm}.

\section{Data collection}
\label{sec:data_collection}

The data collection step is a crucial phase of the entire project. The reason is that the audio record has to be clear, clean and as natural as possible. In fact, the people who participated in this phase were asked to pronounce the sentences as they would say them in a day-by-day conversation. \\

\noindent We recorded 8 people, 4 males and 4 females, at the University of Rochester using \textit{Audacity}\footnote{\url{http://audacityteam.org}}. Each person had to pronounce 10 sentences (see Table~\ref{table:sentences}) and each sentence was pronounced 10 times. \\
\noindent The sentences were chosen in order to cover the most used English sounds and based on the frequencies of everyday usage\footnote{\url{http://www.learn-english-today.com/idioms/idioms_proverbs.html}}.

\begin{table}[!ht]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		\multicolumn{2}{|c|}{Sentences}      \\ \hline
		A piece of cake  & Fair and square   \\ \hline
		Blow a fuse      & Get cold feet     \\ \hline
		Catch some zs    & Mellow out        \\ \hline
		Down to the wire & Pulling your leg  \\ \hline
		Eager beaver     & Thinking out loud \\ \hline
	\end{tabular}
	\caption{Idioms used for testing the pronunciation}
	\label{table:sentences}
\end{table}

\noindent The number of files gathered is 800 and the average length of each file is \textbf{1s}. In total, 14 minutes of recorded audio was gathered. This amount of time was sufficient for training the speech recognition model and the GMM. In reality, for the speech recognition service, the model was initially trained with a bigger dataset and then the sentences were added later (details in Figure~\ref{fig:sphinx_service}). The reason is that the tool used for the speech recognition requires a much larger dataset\footnote{\url{http://cmusphinx.sourceforge.net/wiki/tutorialam}}.

\subsection{Data pre-processing}
\label{ssec:pre_processing}

The data pre-processing step is one of the most important procedures of the whole project. In fact, extracting the right information is crucial for both training the models and those voice-features that should be shown to the user. \\

\noindent The process starts by using the tool called \textbf{PRAAT}\footnote{http://www.fon.hum.uva.nl/praat/}. This tool is used for analysis of speech in phonetics as well as for speech synthesis and articulatory synthesis\cite{boersma2001praat}. PRAAT was used to analyse the audio files we collected in the very beginning of the project and extracting formants and stress, which were described in sections~\ref{sub:formants} and \ref{sec:stress}. \\
From here, a set of CSV files is generated where we saved the values of the formants and the stress for each audio file. These files are then used as input for a tool called \textbf{FAVE-Align}\cite{yuan2008speaker}. \\

\noindent FAVE-Align is a tool used for \textit{force alignment}\footnote{\url{http://www.voxforge.org/home/docs/faq/ faq/what-is-forced-alignment?func=add;class=WebGUI::Asset::Post;withQuote=0.}}. This process is used to determine where a particular word occurs in an audio frame. In other words, FAVE-Align takes a text transcription and produces a PRAAT TextGrid file where it shows when those words start and end in the related audio file \cite{weik2012communications}. The tool performs different phases in order to align audio and text.\\

\noindent The first step is to sample the audio file and apply the Fourier Transformation because there is the need to move from the \textit{time domain} to \textit{frequencies domain}. From here, the tool extracts the \textit{spectrum} and applies the Inverse Fourier Transformation onto it to obtain the so called \textbf{Cepstrum}. The \textit{cepstrum} is the representation in a small-window frame of the spectrum. Although, the amount of information extracted from the cepstrum is too high, and so the tool uses \textit{Perceptual Linear Prediction coefficients} to retrieve the necessary data to perform the alignment decision. These coefficients are used for feature extraction. The detailed process can be found at \cite{hermansky1990perceptual}. \\
The last part of this process is the decision making part and this is done by a \textit{Hidden Markov Model}. \\

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.4]{Figures/fave_align.png}
	\caption{Result from FAVE-Align tool opened in PRAAT}
	\label{fig:fave-align_result}
\end{figure}

\noindent The outcome of the previous step is used as input for the tool called \textbf{FAVE-Extract}. This tool helps to automate the vowel formant analysis. The process is dived in two main steps: the first is finding the \textit{Measurement Points} and the second is the \textit{Remeasurement}. \\

\noindent For most vowels it is possible to find the measurement point by listening 1/3 of the total duration\cite{rosenfelder2011fave}. This point is necessary for determining the identity of the vowel, that is, the name of the vowel itself. For more complex vowels, a different approach is done; that is, the point is halfway between the F1 (main formant) maximum value and the beginning of the segment. In addition, the LPC analysis is performed on both beginning and end of the vowel in order to pad the vowel's window. This is to \emph{ensure a formant track through the full vowel's duration}\cite{harrison2004variability}. The result of this step is a set of candidates. This set is composed of the potential formants estimated from the likelihood of the \textbf{ANAE} distribution. The \textit{Atlas of North American English} (ANAE) is the set of phonology formants values depending on the English regional area. The winner formant is determined by the Posterior probability. This step does not take into consideration the provenience of the speaker. \\

\noindent The second part of the formants extraction tool is to remeasure the parameters by adjusting the ANAE distribution based on the regional area of the speaker. In this way, the formant value will be more accurate. An example of result from FAVE-Extract is shown in Figure~\ref{fig:fave-extract_example}.

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.5]{Figures/fave-extract_example.png}
	\caption{Result from FAVE-Extract}
	\label{fig:fave-extract_example}
\end{figure}

\noindent The result of the data pre-processing is a set of information composed of the average value of F1, F2 and F3 formants with their respectively vowels text representation. The formants values will be then used to train both the speech recognition model and the Gaussian Mixture Model.

\section{Server}
\label{sec:server}

The back-end system is divided in two different services: the first one handles the speech recognition converting the user's voice into a set of phonemes, whereas the second service is in charged of all the other operations a user can do, such as login/logout, history data, vowels prediction system, usage collection, etc.. This section explains more in detail how the information is extracted from the audio files and manipulated before giving the feedback to the user.

\subsection{Speech Recognition service}
\label{ssec:training_sr_model}

The first service in order of usage within the whole system is the speech recognition one. This has been made possible by using the well-known \textbf{CMU-Sphinx} software by Carnegie Mellon University \cite{walker2004sphinx}. The framework is written in Java and it is completely open-source. The system has been deployed on a \textit{Tomcat}\footnote{https://tomcat.apache.org} service as Java Servlet to serve the requests from the Android application. \\

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.6]{Figures/sphinx_service.png}
	\caption{Architecture of the Speech recognition service}
	\label{fig:sphinx_service}
\end{figure}

\noindent The first phase consisted in training the audio model with two different language models. The first (and largest) is the \textit{Generic U.S. English model} whereas the second is composed of the data audio-files collected from the native speakers. The first dataset is directly provided by the tool and already embedded in the decoder. This means that the system has been already trained with a generic model so that new developers do not have to collect data to train the model. This project is a special case because it focuses attention on only 10 specific sentences; in order to specialize the language model, specific files had to be added. This phase took several hours of work because the amount of data used was very large. \\

\noindent Once the model has been trained, the parameters can be adjusted based on the voice of the user. For this task, CMU-Sphinx provides a particular method that permits the model to be adapted based on pitch and speed-of-speech of the user. To do so, the system had to be built in such a way that for each user, a specific file with the voice's parameters was created. In this way, CMU-Sphix would improve the recognition every time a user feeds the system with audio files. \\

\noindent At this point the system is trained and ready to recognize. When the service receives an audio file, the first step before proceeding to CMU-Sphinx is to change some properties of the audio file itself. The Sphinx decoder has the best performance only when the audio files are in \textit{mono-channel} and have a sampling frequency of \textit{16Khz}\footnote{http://cmusphinx.sourceforge.net/wiki/faq}. The library we used to record the user in PARLA, is sampled in \textit{stereo-channels} and \textit{11Khz}. For this reason, a special tool called \textbf{SOX}\footnote{http://sox.sourceforge.net} was used to change the properties of the audio file according to the required ones. \\

\noindent Once the file has been manipulated, the voice's parameters file of the user is retrieved and used to start the recognition part. CMU-Sphinx goes through several internal procedures (general details in chapter~\ref{chap:Speech Recognition}) and during this process it adapts the model based on the user's voice. At the end of the whole process, a string containing the phonemes of the pronounced sentence is given back as result. An example is given in Figure~\ref{fig:result_sphinx}. The red box indicates the result taken into consideration.

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.5]{Figures/result_sphinx.png}
	\caption{Example of phonemes recognition using CMU-Sphinx for the sentence \textit{Thinking out loud}. The phoneme \textbf{SIL} stands for \textit{Silence}}
	\label{fig:result_sphinx}
\end{figure}

\subsection{Voice analysis system}
\label{ssec:training_gmm}

The second service handles the analysis of the audio file in order to give feedback to the user. This process is long because it involves several steps and sometimes the user had to wait up to 40 seconds before receiving the results. Figure~\ref{fig:gmm_service} depicts the macro-view of the service's architecture. \\
\noindent The system was written in Python using Django\footnote{https://www.djangoproject.com} as web-framework. The choice was made based on the availability of machine learning libraries and the language tools (FAVE-extract and FAVE-align). In fact, we used \textit{scikit-learn}\footnote{http://scikit-learn.org/stable/}, a well-known python library for data-analysis, data mining and machine learning. \\

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.6]{Figures/gmm_service.png}
	\caption{Architecture of the Voice analysis service}
	\label{fig:gmm_service}
\end{figure}

\subsection{Training GMM}
\noindent As for the speech recognition service, the Gaussian Mixture Model had to be trained to incorporate the audio features of the native speakers. As explained in section~\ref{ssec:pre_processing}, formants F1, F2 and F3 formed the training dataset for this model. The first three formants are sufficient for recognizing the phoneme that has been pronounced because the first two formants are not enough for discriminating the \textit{"value"} of the phoneme due to a big overlapping in their spectrum. Using the third formants, those frequencies can be caught to act as decision makers\cite{prica2010recognition}. \\

\noindent Scikit-learn provides a GMM out of the box. Although, the number of parameters available make it hard to properly set the model. For this reason, we used a method called \textbf{Bayesian information criterion} (BIC) to find the optimal solution for our purpose. \\

\noindent BIC is a model selection method that gives a score on an estimated model performance based on a testing dataset. The lower the score, the better the model is. \\
\noindent Equation~\ref{eq:bic} is the formula used for calculating the score, where $T$ is the size of the training set and $\ln{\hat L}$ is the maximum likelihood value of the given model (details in section~\ref{sec:mle}), whereas $k$ is the number of \textit{free} parameters that can be estimated. \\

\noindent When the BIC method is attempted, it tries to avoid the risk of \textit{overfitting} the model by injecting a \textit{penalty term} of $k \cdot \ln(T)$ that augment proportionally with the number of parameters\footnote{\url{http://stanfordphd.com/BIC.html}}. This term also helps to avoid unnecessary parameters and keep the model as simple as possible. In Figures~\ref{fig:bic1} and \ref{fig:bic2} of the Appendix, the BIC evaluations are shown.

\begin{equation}
\label{eq:bic}
	 BIC = -2 \cdot \ln{\hat L} + k \cdot \ln(T)
\end{equation}

\noindent Given the results of the evaluation, the model parameters with the lowest BIC score were selected. Listing~\ref{lst:gmm} displays the code used to create the classifier after having run the BIC evaluation.

\begin{lstlisting}[caption={Parameters of GMM classifier},label={lst:gmm}, style=BashInputStyle]
gmm_classifier = mixture.GMM(n_components=12, covariance_type='full',
			     init_params='wmc', min_covar=0.001, n_init=1,
			     n_iter=100, params='wmc', random_state=None,
	                     thresh=None, tol=0.001)
\end{lstlisting}

\clearpage

\noindent The next list of parameters are those that have been automatically selected by the evaluation whereas the others are set by default:

\begin{itemize}
	\item \textit{Number of components} decided based on the total amount of possible phonemes (in our case, 12)
	\item \textit{Covariance type} set to \textbf{full} as indicated by BIC
	\item \textit{Initial parameters} updated by \textbf{weight}(w), \textbf{means}(m) and \textbf{covariance}(c), as indicated by BIC
	\item \textit{Tol} is the convergence threshold. The Expected Maximization breaks when the average gain log-likelihood is below \textbf{0.001}
\end{itemize}

\noindent After the training part, we tested the classifier with a testing set composed by the first 3 Formants that we extracted using PRAAT from 5 audio files provided by the same person. Both \textit{Training accuracy} and \textit{Testing accuracy} were calculated using the function \textbf{numpy.mean()} where the average is computed along the axes that has been specified.

\begin{lstlisting}[caption={Code for accuracy estimation of training and testing set},label={lst:accuracy}, style=BashInputStyle]
train_accuracy = numpy.mean(y_train_predicted == y_train) * 100
test_accuracy = numpy.mean(y_test_predicted == y_test) * 100
\end{lstlisting}

\noindent Table~\ref{table:accuracy} shows the results after the training of Gaussian Mixture Model. The accuracy values can be improved by increasing the amount of training data.

\begin{table}[!ht]
	\centering
	\caption{Testing results after the training}
	\label{table:accuracy}
	\begin{tabular}{|l|c|c|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Sentence}} & \textbf{Training Accuracy}       & \textbf{Testing Accuracy}        \\ \hline
		A piece of cake                         & 								   &
		\\ \cline{1-1}
		Blow a fuse                             &                                  &                                  \\ \cline{1-1}
		Thinking out loud                       &  \textbf{82.5\%}                 & \textbf{90.7\%}
		\\ \cline{1-1}
		Mellow out                              &                                  &                                  \\ \cline{1-1}
		Eager beaver                            &                                  &                                  \\ \hline
	\end{tabular}
\end{table}

\subsection{Pitch, stress and Word Error Rate}
\noindent After the training phase, we built three other components were build to deal directly with the feedback to the user. PRAAT was used to extract the \textit{pitch contour} in order to show the user the way his/her voice changes compared to a native speaker. Figure~\ref{fig:pitch_native} shows an example of contour that was used as feedback for the user. It is possible to notice that both the natives have a similar way of saying the same sentence. This is a key point because the non-native will compare the way he/she will pronounce the sentence and understand the eventual differences. \\

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.15]{Figures/pitch_native.png}
	\caption{Example of pitch contour provided by two native speakers for the sentence \textit{Mellow out}}
	\label{fig:pitch_native}
\end{figure}

\noindent Stress is extracted in a different way. Instead of using PRAAT, \textit{FAVE-extract} was chosen because it provides a feature that retrieves the stress position(s) in the sentence. Moreover, it offers the opportunity to know on which phoneme the stress occurs. Given that, to the user it will be presented the phoneme representation of the pronounced sentence provided by the speech recognition service as well as in which phonemes the stress is emphasized. \\

\noindent The last piece of the system is to calculate the difference between the pronunciation of the native and the user, from the phoneme point of view. For this purpose, a well-known evaluation metric system called \textbf{Word Error Rate} (WER) was used. \\

\noindent WER is an evaluation metric system for checking the accuracy of a speech recognition's system. The main idea is to calculate the distance between the \textbf{hypothesis} and \textbf{reference}. The first is the result produced by the system whereas the second is the expected text. The distance is measured by calculating the minimum number of edits that are needed to change one single character to another. Likewise, WER calculates the minimum amount of operations that have to be done for moving from the reference to the hypothesis. \\ \\ \\ \\

\noindent The possible edits are:

\begin{itemize}
	\item \textit{Insertion}: a word was added to the hypothesis
	\item \textit{Substitution}: an aligned word from the hypothesis has been substituted in the reference
	\item \textit{Deletion}: a word has been deleted in the reference
\end{itemize}

\noindent The calculations are done by putting each edit on a Levenshtein distances table and then \textit{backtracing} in it through the shortest path to the origin $(0, 0)$ \cite{WER}. Each step during the backtrace is counted. After this, WER uses the formula in equation~\ref{eq:wer} to calculate the \textit{error rate}.

\begin{equation}
\label{eq:wer}
	WER = \frac{S + D + I}{N}
\end{equation}

\noindent where S is the substitutions, D the deletions, I the insertions and N are the words in the reference text.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%							ANDROID PART							   %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Android application}
\label{sec:android_app}
\noindent The choice of using the Android OS for developing the mobile application was in part forced by the fact that the other mobile OS do not allow installing applications outside their respective stores. using Android allowed for the unrestricted distribution of the application and for more flexibility regarding the implementation. \\
\noindent We used \textit{Android Studio}\footnote{http://developer.android.com/tools/studio/index.html} as IDE and the \textit{API level 21} where the minimum  Android version required is 5.0.

\subsection{Layouts}
\label{ssec:layouts}
The application is composed of four main layouts: \textit{pronunciation page}, \textit{feedback page}, \textit{history page} and \textit{critical listening page}. Among these, the \textbf{feedback page} is the most important one since it provides the differences with the native pronunciation. \\

\noindent The \textbf{pronunciation page} is depicted in Figure~\ref{fig:pronunciation_page} and it is possible to notice that the user has access to a multitude of options, such as: listening to the native speaker, change word, see the \textit{IPA} phonetics and, of course, test his/her pronunciation. \\

\noindent The \textbf{critical listening} section has been created to help the user to understand the differences between what he/she said and the native, based only on the audio. The page is split in two parts: on the left side there are all the native pronunciations whereas on the right there are all the non-native ones. For each sentence it is then possible to either \textit{test the pronunciation} or \textit{see the history}. The first choice redirects the user to the Main page whereas the second will show the eventual progress for that particular sentence. \\

\begin{figure}[!ht]
	\centering
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[scale=0.18]{Figures/screenshots/main.png}
		\caption{Pronunciation (or Main) page of PARLA}
		\label{fig:pronunciation_page}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[scale=0.18]{Figures/screenshots/listening.png}
		\caption{Listening page}
		\label{fig:listening_page}
	\end{minipage}
\end{figure}

\noindent The \textbf{history page} provides a simple visualization of the user's progress. Figure~\ref{fig:history_page} shows that the page is split in two parts: the top part shows the vowels pronunciation of each time the user tested the pronunciation whereas the bottom part shows how \textit{close} the articulation of that specific vowel was to the native. \\

\noindent These layouts have been designed to provide the necessary information prior testing the pronunciation with the only exception of the history page. As discussed in chapter~\ref{ch:introduction}, \textit{listening} and \textit{phonetics} help the student to improve the quality of the pronunciation as well as the correctness. Keeping these statements in mind, we designed the pages in order to achieve the maximum effect. \\

\noindent The initial development of the User Interface included a simple study wherein a group of 4 people were asked to interact with the sketches of the initial UI. The process was straightforward because the purpose and goals of this application were explained at the beginning of the study. The study was based on a sequence of questions aimed to improve the usability. The investigated topics were:

\begin{itemize}
	\item Navigation among pages
	\item Modifications in the main page
	\item Modifications in the critical listening page
	\item Modifications in the history page
	\item Modifications in the feedback page
	\item Add/Remove features
\end{itemize}

\noindent Based on the answers the layouts were changed accordingly.

\begin{figure}[!ht]
	\centering
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[scale=0.18]{Figures/screenshots/history2.png}
		\caption{Example of History page}
		\label{fig:history2_page}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[scale=0.18]{Figures/screenshots/history.png}
		\caption{History page with interaction}
		\label{fig:history_page}
	\end{minipage}
\end{figure}

\subsection{Feedback layout}
\label{ssec:feedback_layout}
This \textit{feedback page} has been designed to provide as much information as possible giving the minimum explanation regarding the differences. Basically, attention was focused on creating charts and the phonetic representations in order to give feedback. This page combines all the information from both the speech recognition system and the voice analysis service. \\

\noindent The page is divided in three main parts. The first part is represented by the phoneme representation and the WER located at the very top of the page. The button on top left provides the meaning of the sentence as well as a typical usage in a context.\\
\noindent For each sentence we show the comparison between the native and the user. Figures~\ref{fig:error_rate_page} and \ref{fig:error_rate2_page}, show the user immediately can understand those differences, if any. The syllables highlighted in red represent the \textbf{stress} of that particular sentence. In addition, the \textit{word error rate} shows how different the user's pronunciation was from the native speaker's. The second picture shows that the user mispronounced the word \textbf{to}, that is why \textit{WER} is $10\%$. The stress is correct in both cases, otherwise it would be highlighted. \\

\noindent The second part is represented by the chart in which the \textit{stress trend} (or \textit{pitch contour}) is depicted. This chart provides a graphical representation of how the sentence should be emphasized. Similarly to the first part, the difference between the native and the user is displayed. However, the two trends will \textbf{never} be the same because the process of extracting this information involves the \textit{voice pitch} of a person. Basically, what the user should pay attention to, is the \textit{shape} of those lines. If the trends are similar, then it means that the stress is in the right position during the pronunciation. Figure~\ref{fig:pitch_page}, for example, depicts a very bad pronunciation. In fact, WER is $75\%$ and the stress trend does not look like the native's one. The picture clearly shows the impact on a user when making mistakes in the pronunciation. \\

\noindent The last part of this page is represented by the \textit{vowels prediction} chart. Here we show the various pronunciation formants values of the vowels involved in a particular sentence. These values are extracted from the GMM in the voice analysis service. Figure~\ref{fig:vowels_page} shows how the vowels formants are well defined and clustered together. The circles represent the range of formants values in which that determined vowel should be pronounced. The \textit{red crosses} are the user's formants prediction. The feedback information here is that the user should understand that if the red cross is not within the circle and close to the group of green dots, then he/she should change the pronunciation. To improve this there are two methods: using the critical listening or looking at the numbers in the chart. The first method is simpler and more effective whereas the second option is for those that have a prior knowledge in linguistics. \\

\begin{figure}[!ht]
	\centering
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[scale=0.18]{Figures/screenshots/error_rate1.png}
		\caption{Correct pronunciation}
		\label{fig:error_rate_page}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[scale=0.18]{Figures/screenshots/error_rate2.png}
		\caption{Small error in pronunciation}
		\label{fig:error_rate2_page}
	\end{minipage}
\end{figure}

\begin{figure}[!ht]
	\centering
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[scale=0.18]{Figures/screenshots/pitch.png}
		\caption{Stress contour chart}
		\label{fig:pitch_page}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[scale=0.18]{Figures/screenshots/vowels.png}
		\caption{Vowels prediction representation}
		\label{fig:vowels_page}
	\end{minipage}
\end{figure}


\noindent It is important to mention that the user can interact with all the charts. In fact it is possible to zoom in/out and retrieve the value of each single point by tapping on top of the line. This interaction allows users with a linguistic background to have a better numeric-understanding on how the pronunciation was done. \\
\noindent On the very end of this page, the user can navigate directly to the history and see the progress he/she made for that specific sentence.

%%%%% other stuff
